<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>texttunnel: Efficient text processing with GPT-3.5 and GPT-4 &mdash; texttunnel 0.2.1 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="#" class="icon icon-home">
            texttunnel
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">texttunnel: Efficient text processing with GPT-3.5 and GPT-4</a></li>
<li><a class="reference internal" href="#modules">Modules</a><ul>
<li><a class="reference internal" href="#chat-module">Chat Module</a></li>
<li><a class="reference internal" href="#module-texttunnel.models">Models Module</a><ul>
<li><a class="reference internal" href="#texttunnel.models.Model"><code class="docutils literal notranslate"><span class="pre">Model</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.models.Parameters"><code class="docutils literal notranslate"><span class="pre">Parameters</span></code></a><ul>
<li><a class="reference internal" href="#texttunnel.models.Parameters.to_dict"><code class="docutils literal notranslate"><span class="pre">Parameters.to_dict()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#texttunnel.models.GPT_4"><code class="docutils literal notranslate"><span class="pre">models.GPT_4</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.models.GPT_4_0613"><code class="docutils literal notranslate"><span class="pre">models.GPT_4_0613</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.models.GPT_4_32K"><code class="docutils literal notranslate"><span class="pre">models.GPT_4_32K</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.models.GPT_4_32K_0613"><code class="docutils literal notranslate"><span class="pre">models.GPT_4_32K_0613</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.models.GPT_4_0314"><code class="docutils literal notranslate"><span class="pre">models.GPT_4_0314</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.models.GPT_4_32K_0314"><code class="docutils literal notranslate"><span class="pre">models.GPT_4_32K_0314</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.models.GPT_3_5_TURBO"><code class="docutils literal notranslate"><span class="pre">models.GPT_3_5_TURBO</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.models.GPT_3_5_TURBO_16K"><code class="docutils literal notranslate"><span class="pre">models.GPT_3_5_TURBO_16K</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.models.GPT_3_5_TURBO_0613"><code class="docutils literal notranslate"><span class="pre">models.GPT_3_5_TURBO_0613</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.models.GPT_3_5_TURBO_16K_0613"><code class="docutils literal notranslate"><span class="pre">models.GPT_3_5_TURBO_16K_0613</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.models.GPT_3_5_TURBO_0301"><code class="docutils literal notranslate"><span class="pre">models.GPT_3_5_TURBO_0301</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#processor-module">Processor Module</a></li>
<li><a class="reference internal" href="#utils-module">Utils Module</a></li>
</ul>
</li>
<li><a class="reference internal" href="#logging">Logging</a></li>
<li><a class="reference internal" href="#indices-and-tables">Indices and tables</a></li>
</ul>
</div>
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">texttunnel</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="#" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">texttunnel: Efficient text processing with GPT-3.5 and GPT-4</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="texttunnel-efficient-text-processing-with-gpt-3-5-and-gpt-4">
<h1>texttunnel: Efficient text processing with GPT-3.5 and GPT-4<a class="headerlink" href="#texttunnel-efficient-text-processing-with-gpt-3-5-and-gpt-4" title="Permalink to this heading">ÔÉÅ</a></h1>
<p>This package offers a straightforward interface for integrating the GPT-3.5 and GPT-4 models into your natural language processing pipelines. It is optimally designed for the following scenario:</p>
<p>Suppose you possess a corpus of text data that you want to analyze using the GPT-3.5 or GPT-4 models. The goal is to perform extractive NLP tasks such as classification, named entity recognition, translation, summarization, question answering, or sentiment analysis. In this context, the package prioritizes efficiency and tidiness to provide you streamlined results.</p>
<p>Features:</p>
<ul class="simple">
<li><p>üìÑ Output Schema: Utilizes JSON Schema alongside OpenAI‚Äôs function calling schema to define the output data structure.</p></li>
<li><p>‚úîÔ∏è Input Validation: Ensures well-structured and error-free API requests by validating input data.</p></li>
<li><p>‚úÖ Output Validation: Checks the response data from OpenAI‚Äôs API against the expected schema to maintain data integrity.</p></li>
<li><p>üöÄ Efficient Batching: Supports bulk processing by packing multiple input texts into a single request for the OpenAI‚Äôs API.</p></li>
<li><p>üö¶ Asynchronous Requests: Facilitates speedy data processing by sending simultaneous requests to OpenAI‚Äôs API, while maintaining API rate limits.</p></li>
<li><p>üí∞ Cost Estimation: Aims for transparency in API utilization cost by providing cost estimates before sending API requests.</p></li>
<li><p>üíæ Caching: Uses aiohttp_client_cach to avoid redundant requests and reduce cost by caching previous requests. Supports SQLite, MongoDB, DynamoDB and Redis cache backends.</p></li>
<li><p>üìù Request Logging: Implements Python‚Äôs native logging framework for tracking and logging all API requests.</p></li>
</ul>
<p>To get started, check the examples:
<a class="reference external" href="https://github.com/qagentur/texttunnel/tree/main/examples">https://github.com/qagentur/texttunnel/tree/main/examples</a></p>
<p>OpenAI‚Äôs function calling guide is also a useful resource:
<a class="reference external" href="https://platform.openai.com/docs/guides/gpt/function-calling">https://platform.openai.com/docs/guides/gpt/function-calling</a></p>
<div class="toctree-wrapper compound">
</div>
</section>
<section id="modules">
<h1>Modules<a class="headerlink" href="#modules" title="Permalink to this heading">ÔÉÅ</a></h1>
<section id="chat-module">
<h2>Chat Module<a class="headerlink" href="#chat-module" title="Permalink to this heading">ÔÉÅ</a></h2>
</section>
<section id="module-texttunnel.models">
<span id="models-module"></span><h2>Models Module<a class="headerlink" href="#module-texttunnel.models" title="Permalink to this heading">ÔÉÅ</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="texttunnel.models.Model">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">texttunnel.models.</span></span><span class="sig-name descname"><span class="pre">Model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">context_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_token_price_per_1k</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_token_price_per_1k</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokens_per_minute</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requests_per_minute</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#texttunnel.models.Model" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Information about an OpenAI ChatCompletion model.
Check prices here: <a class="reference external" href="https://openai.com/pricing">https://openai.com/pricing</a></p>
<p>Note that rate limits differ between OpenAI accounts.
Check them here: <a class="reference external" href="https://platform.openai.com/account/rate-limits">https://platform.openai.com/account/rate-limits</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> ‚Äì The name of the model, e.g. ‚Äúgpt-3.5-turbo‚Äù.</p></li>
<li><p><strong>context_size</strong> ‚Äì The maximum number of tokens that can be passed to the model.</p></li>
<li><p><strong>input_token_price_per_1k</strong> ‚Äì The price in USD per 1000 tokens for input.</p></li>
<li><p><strong>output_token_price_per_1k</strong> ‚Äì The price in USD per 1000 tokens for output.</p></li>
<li><p><strong>tokens_per_minute</strong> ‚Äì The maximum number of tokens that can be processed per minute.
Note that this may differ between OpenAI accounts. Override the default
models‚Äô values with your own values.</p></li>
<li><p><strong>requests_per_minute</strong> ‚Äì The maximum number of requests that can be made per minute.
Note that this may differ between OpenAI accounts. Override the default
models‚Äô values with your own values.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="texttunnel.models.Parameters">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">texttunnel.models.</span></span><span class="sig-name descname"><span class="pre">Parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">max_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">presence_penalty</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">frequency_penalty</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">42</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#texttunnel.models.Parameters" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Set of parameters that can be passed to an API request.</p>
<p>The parameters are explained in the OpenAI API documentation:
<a class="reference external" href="https://platform.openai.com/docs/api-reference/chat/create">https://platform.openai.com/docs/api-reference/chat/create</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>max_tokens</strong> ‚Äì The maximum number of tokens to generate. Note:
This can‚Äôt be greater than the model‚Äôs context size and should be at least
long enough to fit the whole expected JSON output. This parameter is used
to estimate the cost of the request.</p></li>
<li><p><strong>temperature</strong> ‚Äì What sampling temperature to use, between 0 and 2.
Higher values like 0.8 will make the output more random, while
lower values like 0.2 will make it more focused and deterministic.
Defaults to 0.0 because this package is designed for deterministic
JSON-schema compliant output.</p></li>
<li><p><strong>presence_penalty</strong> ‚Äì Number between -2.0 and 2.0. Positive values penalize
new tokens based on whether they appear in the text so far,
increasing the model‚Äôs likelihood to talk about new topics. Defaults to 0.0.</p></li>
<li><p><strong>frequency_penalty</strong> ‚Äì Number between -2.0 and 2.0. Positive values penalize
new tokens based on their existing frequency in the text so far,
decreasing the model‚Äôs likelihood to repeat the same line verbatim.
Defaults to 0.0.</p></li>
<li><p><strong>seed</strong> ‚Äì Integer seed for random number generation. Defaults to 42.</p></li>
</ul>
</dd>
</dl>
<p>Parameters that are not listed here are not supported by this package. The
reason is that they‚Äôre not relevant for the use case of this package.</p>
<dl class="py method">
<dt class="sig sig-object py" id="texttunnel.models.Parameters.to_dict">
<span class="sig-name descname"><span class="pre">to_dict</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#texttunnel.models.Parameters.to_dict" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dictionary representation of the parameters.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="texttunnel.models.GPT_4">
<span class="sig-prename descclassname"><span class="pre">models.</span></span><span class="sig-name descname"><span class="pre">GPT_4</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">Model(name='gpt-4',</span> <span class="pre">context_size=8192,</span> <span class="pre">input_token_price_per_1k=0.03,</span> <span class="pre">output_token_price_per_1k=0.06,</span> <span class="pre">tokens_per_minute=10000,</span> <span class="pre">requests_per_minute=500)</span></em><a class="headerlink" href="#texttunnel.models.GPT_4" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="texttunnel.models.GPT_4_0613">
<span class="sig-prename descclassname"><span class="pre">models.</span></span><span class="sig-name descname"><span class="pre">GPT_4_0613</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">Model(name='gpt-4-0613',</span> <span class="pre">context_size=8192,</span> <span class="pre">input_token_price_per_1k=0.03,</span> <span class="pre">output_token_price_per_1k=0.06,</span> <span class="pre">tokens_per_minute=10000,</span> <span class="pre">requests_per_minute=500)</span></em><a class="headerlink" href="#texttunnel.models.GPT_4_0613" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="texttunnel.models.GPT_4_32K">
<span class="sig-prename descclassname"><span class="pre">models.</span></span><span class="sig-name descname"><span class="pre">GPT_4_32K</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">Model(name='gpt-4-32k',</span> <span class="pre">context_size=32768,</span> <span class="pre">input_token_price_per_1k=0.06,</span> <span class="pre">output_token_price_per_1k=0.12,</span> <span class="pre">tokens_per_minute=20000,</span> <span class="pre">requests_per_minute=500)</span></em><a class="headerlink" href="#texttunnel.models.GPT_4_32K" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="texttunnel.models.GPT_4_32K_0613">
<span class="sig-prename descclassname"><span class="pre">models.</span></span><span class="sig-name descname"><span class="pre">GPT_4_32K_0613</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">Model(name='gpt-4-32k-0613',</span> <span class="pre">context_size=32768,</span> <span class="pre">input_token_price_per_1k=0.06,</span> <span class="pre">output_token_price_per_1k=0.12,</span> <span class="pre">tokens_per_minute=20000,</span> <span class="pre">requests_per_minute=500)</span></em><a class="headerlink" href="#texttunnel.models.GPT_4_32K_0613" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="texttunnel.models.GPT_4_0314">
<span class="sig-prename descclassname"><span class="pre">models.</span></span><span class="sig-name descname"><span class="pre">GPT_4_0314</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">Model(name='gpt-4-0314',</span> <span class="pre">context_size=8192,</span> <span class="pre">input_token_price_per_1k=0.03,</span> <span class="pre">output_token_price_per_1k=0.06,</span> <span class="pre">tokens_per_minute=10000,</span> <span class="pre">requests_per_minute=500)</span></em><a class="headerlink" href="#texttunnel.models.GPT_4_0314" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="texttunnel.models.GPT_4_32K_0314">
<span class="sig-prename descclassname"><span class="pre">models.</span></span><span class="sig-name descname"><span class="pre">GPT_4_32K_0314</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">Model(name='gpt-4-32k-0314',</span> <span class="pre">context_size=32768,</span> <span class="pre">input_token_price_per_1k=0.06,</span> <span class="pre">output_token_price_per_1k=0.12,</span> <span class="pre">tokens_per_minute=10000,</span> <span class="pre">requests_per_minute=500)</span></em><a class="headerlink" href="#texttunnel.models.GPT_4_32K_0314" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="texttunnel.models.GPT_3_5_TURBO">
<span class="sig-prename descclassname"><span class="pre">models.</span></span><span class="sig-name descname"><span class="pre">GPT_3_5_TURBO</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">Model(name='gpt-3.5-turbo',</span> <span class="pre">context_size=4096,</span> <span class="pre">input_token_price_per_1k=0.0015,</span> <span class="pre">output_token_price_per_1k=0.002,</span> <span class="pre">tokens_per_minute=90000,</span> <span class="pre">requests_per_minute=3500)</span></em><a class="headerlink" href="#texttunnel.models.GPT_3_5_TURBO" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="texttunnel.models.GPT_3_5_TURBO_16K">
<span class="sig-prename descclassname"><span class="pre">models.</span></span><span class="sig-name descname"><span class="pre">GPT_3_5_TURBO_16K</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">Model(name='gpt-3.5-turbo-16k',</span> <span class="pre">context_size=16384,</span> <span class="pre">input_token_price_per_1k=0.003,</span> <span class="pre">output_token_price_per_1k=0.004,</span> <span class="pre">tokens_per_minute=180000,</span> <span class="pre">requests_per_minute=3500)</span></em><a class="headerlink" href="#texttunnel.models.GPT_3_5_TURBO_16K" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="texttunnel.models.GPT_3_5_TURBO_0613">
<span class="sig-prename descclassname"><span class="pre">models.</span></span><span class="sig-name descname"><span class="pre">GPT_3_5_TURBO_0613</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">Model(name='gpt-3.5-turbo-0613',</span> <span class="pre">context_size=4096,</span> <span class="pre">input_token_price_per_1k=0.0015,</span> <span class="pre">output_token_price_per_1k=0.002,</span> <span class="pre">tokens_per_minute=90000,</span> <span class="pre">requests_per_minute=3500)</span></em><a class="headerlink" href="#texttunnel.models.GPT_3_5_TURBO_0613" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="texttunnel.models.GPT_3_5_TURBO_16K_0613">
<span class="sig-prename descclassname"><span class="pre">models.</span></span><span class="sig-name descname"><span class="pre">GPT_3_5_TURBO_16K_0613</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">Model(name='gpt-3.5-turbo-16k-0613',</span> <span class="pre">context_size=16384,</span> <span class="pre">input_token_price_per_1k=0.003,</span> <span class="pre">output_token_price_per_1k=0.004,</span> <span class="pre">tokens_per_minute=180000,</span> <span class="pre">requests_per_minute=3500)</span></em><a class="headerlink" href="#texttunnel.models.GPT_3_5_TURBO_16K_0613" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="texttunnel.models.GPT_3_5_TURBO_0301">
<span class="sig-prename descclassname"><span class="pre">models.</span></span><span class="sig-name descname"><span class="pre">GPT_3_5_TURBO_0301</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">Model(name='gpt-3.5-turbo-0301',</span> <span class="pre">context_size=4096,</span> <span class="pre">input_token_price_per_1k=0.0015,</span> <span class="pre">output_token_price_per_1k=0.002,</span> <span class="pre">tokens_per_minute=9000,</span> <span class="pre">requests_per_minute=3500)</span></em><a class="headerlink" href="#texttunnel.models.GPT_3_5_TURBO_0301" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd></dd></dl>

<p>Models that are not included here can be created as custom instances of the Model class. Only chat models are supported; ‚Äúinstruct‚Äù models are not supported.</p>
<p>Preview models can be used, but will not be added as default models to the package. To use a preview model, create a custom instance of the Model class. Models that OpenAI deprecates will be removed from the package. This primarily affects date-versioned models.</p>
<p>Note that the model class attributes tokens_per_minute (TPM) and requests_per_minute (RPM) are based on tier 1 usage limits. See <a class="reference external" href="https://platform.openai.com/docs/guides/rate-limits?context=tier-free">https://platform.openai.com/docs/guides/rate-limits?context=tier-free</a> for more details. If your account has a higher usage tier, override the class attributes with your own values.</p>
<p>texttunnel does not track tokens_per_day (TPD) limits and assumes that it is the only process that is using your model quota.</p>
</section>
<section id="processor-module">
<h2>Processor Module<a class="headerlink" href="#processor-module" title="Permalink to this heading">ÔÉÅ</a></h2>
</section>
<section id="utils-module">
<h2>Utils Module<a class="headerlink" href="#utils-module" title="Permalink to this heading">ÔÉÅ</a></h2>
</section>
</section>
<section id="logging">
<h1>Logging<a class="headerlink" href="#logging" title="Permalink to this heading">ÔÉÅ</a></h1>
<p>The package uses the standard logging library and creates a logger named ‚Äútexttunnel‚Äù.</p>
<p>To enable logging, add the following code to your script:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">WARNING</span><span class="p">)</span> <span class="c1"># choose whatever level you want</span>
<span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;texttunnel&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span> <span class="c1"># set to DEBUG for more verbose logging</span>
</pre></div>
</div>
</section>
<section id="indices-and-tables">
<h1>Indices and tables<a class="headerlink" href="#indices-and-tables" title="Permalink to this heading">ÔÉÅ</a></h1>
<ul class="simple">
<li><p><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></p></li>
<li><p><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></p></li>
<li><p><a class="reference internal" href="search.html"><span class="std std-ref">Search Page</span></a></p></li>
</ul>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Q Agentur f√ºr Forschung GmbH.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>