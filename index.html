<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Welcome to texttunnelâ€™s documentation! &mdash; texttunnel 0.2.1 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="#" class="icon icon-home">
            texttunnel
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">Welcome to texttunnelâ€™s documentation!</a><ul>
<li><a class="reference internal" href="#modules">Modules</a><ul>
<li><a class="reference internal" href="#module-texttunnel.chat">Chat Module</a><ul>
<li><a class="reference internal" href="#texttunnel.chat.Chat"><code class="docutils literal notranslate"><span class="pre">Chat</span></code></a><ul>
<li><a class="reference internal" href="#texttunnel.chat.Chat.add_message"><code class="docutils literal notranslate"><span class="pre">Chat.add_message()</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.chat.Chat.count_tokens"><code class="docutils literal notranslate"><span class="pre">Chat.count_tokens()</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.chat.Chat.to_list"><code class="docutils literal notranslate"><span class="pre">Chat.to_list()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#texttunnel.chat.ChatCompletionRequest"><code class="docutils literal notranslate"><span class="pre">ChatCompletionRequest</span></code></a><ul>
<li><a class="reference internal" href="#texttunnel.chat.ChatCompletionRequest.count_input_tokens"><code class="docutils literal notranslate"><span class="pre">ChatCompletionRequest.count_input_tokens()</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.chat.ChatCompletionRequest.count_output_tokens"><code class="docutils literal notranslate"><span class="pre">ChatCompletionRequest.count_output_tokens()</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.chat.ChatCompletionRequest.count_total_tokens"><code class="docutils literal notranslate"><span class="pre">ChatCompletionRequest.count_total_tokens()</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.chat.ChatCompletionRequest.estimate_cost_usd"><code class="docutils literal notranslate"><span class="pre">ChatCompletionRequest.estimate_cost_usd()</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.chat.ChatCompletionRequest.get_hash"><code class="docutils literal notranslate"><span class="pre">ChatCompletionRequest.get_hash()</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.chat.ChatCompletionRequest.to_dict"><code class="docutils literal notranslate"><span class="pre">ChatCompletionRequest.to_dict()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#texttunnel.chat.ChatMessage"><code class="docutils literal notranslate"><span class="pre">ChatMessage</span></code></a><ul>
<li><a class="reference internal" href="#texttunnel.chat.ChatMessage.to_dict"><code class="docutils literal notranslate"><span class="pre">ChatMessage.to_dict()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#texttunnel.chat.build_binpacked_requests"><code class="docutils literal notranslate"><span class="pre">build_binpacked_requests()</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.chat.build_requests"><code class="docutils literal notranslate"><span class="pre">build_requests()</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.chat.is_valid_function_def"><code class="docutils literal notranslate"><span class="pre">is_valid_function_def()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#module-texttunnel.models">Models Module</a><ul>
<li><a class="reference internal" href="#texttunnel.models.Model"><code class="docutils literal notranslate"><span class="pre">Model</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.models.GPT_4"><code class="docutils literal notranslate"><span class="pre">models.GPT_4</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.models.GPT_4_0613"><code class="docutils literal notranslate"><span class="pre">models.GPT_4_0613</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.models.GPT_4_32K"><code class="docutils literal notranslate"><span class="pre">models.GPT_4_32K</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.models.GPT_4_32K_0613"><code class="docutils literal notranslate"><span class="pre">models.GPT_4_32K_0613</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.models.GPT_4_0314"><code class="docutils literal notranslate"><span class="pre">models.GPT_4_0314</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.models.GPT_4_32K_0314"><code class="docutils literal notranslate"><span class="pre">models.GPT_4_32K_0314</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.models.GPT_3_5_TURBO"><code class="docutils literal notranslate"><span class="pre">models.GPT_3_5_TURBO</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.models.GPT_3_5_TURBO_16K"><code class="docutils literal notranslate"><span class="pre">models.GPT_3_5_TURBO_16K</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.models.GPT_3_5_TURBO_0613"><code class="docutils literal notranslate"><span class="pre">models.GPT_3_5_TURBO_0613</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.models.GPT_3_5_TURBO_16K_0613"><code class="docutils literal notranslate"><span class="pre">models.GPT_3_5_TURBO_16K_0613</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.models.GPT_3_5_TURBO_0301"><code class="docutils literal notranslate"><span class="pre">models.GPT_3_5_TURBO_0301</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#module-texttunnel.processor">Processor Module</a><ul>
<li><a class="reference internal" href="#texttunnel.processor.APIRequest"><code class="docutils literal notranslate"><span class="pre">APIRequest</span></code></a><ul>
<li><a class="reference internal" href="#texttunnel.processor.APIRequest.call_api"><code class="docutils literal notranslate"><span class="pre">APIRequest.call_api()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#texttunnel.processor.StatusTracker"><code class="docutils literal notranslate"><span class="pre">StatusTracker</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.processor.append_to_jsonl"><code class="docutils literal notranslate"><span class="pre">append_to_jsonl()</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.processor.aprocess_api_requests"><code class="docutils literal notranslate"><span class="pre">aprocess_api_requests()</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.processor.is_valid_response"><code class="docutils literal notranslate"><span class="pre">is_valid_response()</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.processor.parse_arguments"><code class="docutils literal notranslate"><span class="pre">parse_arguments()</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.processor.parse_token_usage"><code class="docutils literal notranslate"><span class="pre">parse_token_usage()</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.processor.prepare_output_filepath"><code class="docutils literal notranslate"><span class="pre">prepare_output_filepath()</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.processor.process_api_requests"><code class="docutils literal notranslate"><span class="pre">process_api_requests()</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.processor.task_id_generator_function"><code class="docutils literal notranslate"><span class="pre">task_id_generator_function()</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.processor.usage_to_cost"><code class="docutils literal notranslate"><span class="pre">usage_to_cost()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#module-texttunnel.utils">Utils Module</a><ul>
<li><a class="reference internal" href="#texttunnel.utils.binpack_texts_in_order"><code class="docutils literal notranslate"><span class="pre">binpack_texts_in_order()</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.utils.format_texts_as_json"><code class="docutils literal notranslate"><span class="pre">format_texts_as_json()</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.utils.format_texts_with_spaces"><code class="docutils literal notranslate"><span class="pre">format_texts_with_spaces()</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.utils.hash_dict"><code class="docutils literal notranslate"><span class="pre">hash_dict()</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.utils.num_tokens_from_text"><code class="docutils literal notranslate"><span class="pre">num_tokens_from_text()</span></code></a></li>
<li><a class="reference internal" href="#texttunnel.utils.truncate_text_by_tokens"><code class="docutils literal notranslate"><span class="pre">truncate_text_by_tokens()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#indices-and-tables">Indices and tables</a></li>
</ul>
</div>
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">texttunnel</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="#" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Welcome to texttunnelâ€™s documentation!</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="welcome-to-texttunnel-s-documentation">
<h1>Welcome to texttunnelâ€™s documentation!<a class="headerlink" href="#welcome-to-texttunnel-s-documentation" title="Permalink to this heading">ïƒ</a></h1>
<p>his package offers a straightforward interface for integrating the GPT-3.5 and GPT-4 models into your natural language processing pipelines. It is optimally designed for the following scenario:</p>
<p>Suppose you possess a corpus of text data that you want to analyze using the GPT-3.5 or GPT-4 models. The goal is to perform extractive NLP tasks such as classification, named entity recognition, translation, summarization, question answering, or sentiment analysis. In this context, the package prioritizes efficiency and tidiness to provide you streamlined results.</p>
<p>Features:</p>
<ul class="simple">
<li><p>ğŸ“„ Output Schema: Utilizes JSON Schema alongside OpenAIâ€™s function calling schema to define the output data structure.</p></li>
<li><p>âœ”ï¸ Input Validation: Ensures well-structured and error-free API requests by validating input data.</p></li>
<li><p>âœ… Output Validation: Checks the response data from OpenAIâ€™s API against the expected schema to maintain data integrity.</p></li>
<li><p>ğŸš€ Efficient Batching: Supports bulk processing by packing multiple input texts into a single request for the OpenAIâ€™s API.</p></li>
<li><p>ğŸš¦ Asynchronous Requests: Facilitates speedy data processing by sending simultaneous requests to OpenAIâ€™s API, while maintaining API rate limits.</p></li>
<li><p>ğŸ’° Cost Estimation: Aims for transparency in API utilization cost by providing cost estimates before sending API requests.</p></li>
<li><p>ğŸ’¾ Disk Caching: Uses diskcache to avoid redundant requests and reduce cost by caching previous requests.</p></li>
<li><p>ğŸ“ Request Logging: Implements Pythonâ€™s native logging framework for tracking and logging all API requests.</p></li>
</ul>
<p>To get started, check the examples:
<a class="reference external" href="https://github.com/qagentur/texttunnel/tree/main/examples">https://github.com/qagentur/texttunnel/tree/main/examples</a></p>
<div class="toctree-wrapper compound">
</div>
<section id="modules">
<h2>Modules<a class="headerlink" href="#modules" title="Permalink to this heading">ïƒ</a></h2>
<section id="module-texttunnel.chat">
<span id="chat-module"></span><h3>Chat Module<a class="headerlink" href="#module-texttunnel.chat" title="Permalink to this heading">ïƒ</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="texttunnel.chat.Chat">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">texttunnel.chat.</span></span><span class="sig-name descname"><span class="pre">Chat</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">messages</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#texttunnel.chat.ChatMessage" title="texttunnel.chat.ChatMessage"><span class="pre">ChatMessage</span></a><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#texttunnel.chat.Chat" title="Permalink to this definition">ïƒ</a></dt>
<dd><p>A chat. Used to prompt a model for a response.
The first message must be from the system, and the last message must be from the user.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>messages</strong> â€“ A list of ChatMessage objects.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="texttunnel.chat.Chat.add_message">
<span class="sig-name descname"><span class="pre">add_message</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">message</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#texttunnel.chat.ChatMessage" title="texttunnel.chat.ChatMessage"><span class="pre">ChatMessage</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#texttunnel.chat.Chat.add_message" title="Permalink to this definition">ïƒ</a></dt>
<dd><p>Adds a message to the end of the chat.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>message</strong> â€“ The message to add.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="texttunnel.chat.Chat.count_tokens">
<span class="sig-name descname"><span class="pre">count_tokens</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'gpt-3.5-turbo-0613'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">show_changing_model_warning</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#texttunnel.chat.Chat.count_tokens" title="Permalink to this definition">ïƒ</a></dt>
<dd><p>Return the number of tokens used.
Note that this depends on the model used. Models that are not versioned
with a date can change over time, causing an inaccurate token count
by this function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> â€“ The name of the model to use. Defaults to â€œgpt-3.5-turbo-0613â€.</p></li>
<li><p><strong>show_changing_model_warning</strong> â€“ Whether to print a warning if a model
is used that may change over time. Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The number of tokens used.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="texttunnel.chat.Chat.to_list">
<span class="sig-name descname"><span class="pre">to_list</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#texttunnel.chat.Chat.to_list" title="Permalink to this definition">ïƒ</a></dt>
<dd><p>Returns a list of dictionaries representing the chat messages.
This is the format expected by the OpenAI API.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="texttunnel.chat.ChatCompletionRequest">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">texttunnel.chat.</span></span><span class="sig-name descname"><span class="pre">ChatCompletionRequest</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">chat</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#texttunnel.chat.Chat" title="texttunnel.chat.Chat"><span class="pre">Chat</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#texttunnel.models.Model" title="texttunnel.models.Model"><span class="pre">Model</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">function</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_output_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_params</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#texttunnel.chat.ChatCompletionRequest" title="Permalink to this definition">ïƒ</a></dt>
<dd><p>Defines a request for a chat completion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>chat</strong> â€“ The chat to which the assistant should respond with a function call.</p></li>
<li><p><strong>model</strong> â€“ The name of the OpenAI ChatCompletion model to use for completion.</p></li>
<li><p><strong>function</strong> â€“ The function definition to use for the assistantâ€™s response.
Must be a dictionary that describes a valid JSON schema.
See <a class="reference external" href="https://platform.openai.com/docs/guides/gpt/function-calling">https://platform.openai.com/docs/guides/gpt/function-calling</a></p></li>
<li><p><strong>max_output_tokens</strong> â€“ The maximum number of tokens allowed in the completion.
Defaults to 128.</p></li>
<li><p><strong>model_params</strong> â€“ Additional keyword arguments to pass to the OpenAI API. See
<a class="reference external" href="https://platform.openai.com/docs/api-reference/completions/create">https://platform.openai.com/docs/api-reference/completions/create</a></p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="texttunnel.chat.ChatCompletionRequest.count_input_tokens">
<span class="sig-name descname"><span class="pre">count_input_tokens</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#texttunnel.chat.ChatCompletionRequest.count_input_tokens" title="Permalink to this definition">ïƒ</a></dt>
<dd><p>Counts the number of tokens that will be used as input to the model.
This includes the chat messages and the function call.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="texttunnel.chat.ChatCompletionRequest.count_output_tokens">
<span class="sig-name descname"><span class="pre">count_output_tokens</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#texttunnel.chat.ChatCompletionRequest.count_output_tokens" title="Permalink to this definition">ïƒ</a></dt>
<dd><p>Counts the number of tokens that will be used as output of the model.
Assumes that the model will return the maximum number of tokens allowed
by the max_tokens parameter.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="texttunnel.chat.ChatCompletionRequest.count_total_tokens">
<span class="sig-name descname"><span class="pre">count_total_tokens</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#texttunnel.chat.ChatCompletionRequest.count_total_tokens" title="Permalink to this definition">ïƒ</a></dt>
<dd><p>Counts the total number of tokens that will be used as input and output
of the model. Assumes that the model will return the maximum number of
tokens allowed by the max_tokens parameter.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="texttunnel.chat.ChatCompletionRequest.estimate_cost_usd">
<span class="sig-name descname"><span class="pre">estimate_cost_usd</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="headerlink" href="#texttunnel.chat.ChatCompletionRequest.estimate_cost_usd" title="Permalink to this definition">ïƒ</a></dt>
<dd><p>Estimates the cost of the request in USD. Assumes that the model will
return the maximum number of tokens allowed by the max_tokens parameter.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="texttunnel.chat.ChatCompletionRequest.get_hash">
<span class="sig-name descname"><span class="pre">get_hash</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#texttunnel.chat.ChatCompletionRequest.get_hash" title="Permalink to this definition">ïƒ</a></dt>
<dd><p>Returns the hash of the request. Can be used as a cache key.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="texttunnel.chat.ChatCompletionRequest.to_dict">
<span class="sig-name descname"><span class="pre">to_dict</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#texttunnel.chat.ChatCompletionRequest.to_dict" title="Permalink to this definition">ïƒ</a></dt>
<dd><p>Returns a dictionary representation of the request. Only includes
the elements that are required by the OpenAI API. Model parameters
are flattened into the top-level dictionary.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="texttunnel.chat.ChatMessage">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">texttunnel.chat.</span></span><span class="sig-name descname"><span class="pre">ChatMessage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">role</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">content</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#texttunnel.chat.ChatMessage" title="Permalink to this definition">ïƒ</a></dt>
<dd><p>A chat message, to be used in a chat.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>role</strong> â€“ The role of the message. Must be one of â€œsystemâ€, â€œuserâ€, or â€œassistantâ€.</p></li>
<li><p><strong>content</strong> â€“ The content of the message.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="texttunnel.chat.ChatMessage.to_dict">
<span class="sig-name descname"><span class="pre">to_dict</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#texttunnel.chat.ChatMessage.to_dict" title="Permalink to this definition">ïƒ</a></dt>
<dd><p>Returns a dict representation of the message.</p>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="texttunnel.chat.build_binpacked_requests">
<span class="sig-prename descclassname"><span class="pre">texttunnel.chat.</span></span><span class="sig-name descname"><span class="pre">build_binpacked_requests</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">model:</span> <span class="pre">~texttunnel.models.Model,</span> <span class="pre">function:</span> <span class="pre">~typing.Dict[str,</span> <span class="pre">str],</span> <span class="pre">system_message:</span> <span class="pre">str,</span> <span class="pre">texts:</span> <span class="pre">~typing.List[str],</span> <span class="pre">max_tokens_per_request:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">max_texts_per_request:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">max_output_tokens:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">128,</span> <span class="pre">binpacking_function:</span> <span class="pre">~typing.Callable</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">binpack_texts_in_order&gt;,</span> <span class="pre">formatter_function:</span> <span class="pre">~typing.Callable</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">format_texts_as_json&gt;,</span> <span class="pre">encoding_name:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'cl100k_base',</span> <span class="pre">long_text_handling:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'error',</span> <span class="pre">model_params:</span> <span class="pre">~typing.Dict[str,</span> <span class="pre">~typing.Any]</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#texttunnel.chat.ChatCompletionRequest" title="texttunnel.chat.ChatCompletionRequest"><span class="pre">ChatCompletionRequest</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#texttunnel.chat.build_binpacked_requests" title="Permalink to this definition">ïƒ</a></dt>
<dd><p>Builds a list of ChatCompletionRequests from a list of texts.
If possible, multiple texts will be combined into a single ChatCompletionRequest.
This can reduce the number of tokens spent on overheads like the system message
and function definition.</p>
<p>The requests can then be passed to processor.process_api_requests().</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> â€“ The model to use for completion.</p></li>
<li><p><strong>function</strong> â€“ The function definition to use for the assistantâ€™s response.
Must be a dictionary that describes a valid JSON schema.
See <a class="reference external" href="https://platform.openai.com/docs/guides/gpt/function-calling">https://platform.openai.com/docs/guides/gpt/function-calling</a></p></li>
<li><p><strong>system_message</strong> â€“ The message to include at the beginning of each chat.</p></li>
<li><p><strong>texts</strong> â€“ A list of texts to binpack into chats.</p></li>
<li><p><strong>max_tokens_per_request</strong> â€“ The maximum number of tokens allowed in one request.
Defaults to 90% of the modelâ€™s context size. The 10% buffer makes
sure that mistakes in token counting donâ€™t cause the request to fail.</p></li>
<li><p><strong>max_texts_per_request</strong> â€“ The maximum number of texts allowed in one request.
Defaults to None, which means there is no limit.</p></li>
<li><p><strong>max_output_tokens</strong> â€“ The maximum number of tokens allowed in the completion.</p></li>
<li><p><strong>binpacking_function</strong> â€“ The function to use for binpacking.
Must take a list of texts and return a list of lists of texts.
Defaults to binpack_texts_in_order().</p></li>
<li><p><strong>formatter_function</strong> â€“ The function to use for formatting the texts.
Must take a list of texts and return a single string.
Defaults to format_texts_as_json().</p></li>
<li><p><strong>encoding_name</strong> â€“ The name of the encoding to use for tokenization.
Defaults to â€œcl100k_baseâ€.</p></li>
<li><p><strong>long_text_handling</strong> â€“ Passed to the binpacking function. Defaults to
â€œerrorâ€, which means that an error will be raised if a text is too
long to fit in a single chat.</p></li>
<li><p><strong>model_params</strong> â€“ Additional keyword arguments to pass to the OpenAI API. See
<a class="reference external" href="https://platform.openai.com/docs/api-reference/completions/create">https://platform.openai.com/docs/api-reference/completions/create</a></p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A list of ChatCompletionRequests.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="texttunnel.chat.build_requests">
<span class="sig-prename descclassname"><span class="pre">texttunnel.chat.</span></span><span class="sig-name descname"><span class="pre">build_requests</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#texttunnel.models.Model" title="texttunnel.models.Model"><span class="pre">Model</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">function</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">system_message</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">texts</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_output_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoding_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'cl100k_base'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">long_text_handling</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'error'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_params</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#texttunnel.chat.ChatCompletionRequest" title="texttunnel.chat.ChatCompletionRequest"><span class="pre">ChatCompletionRequest</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#texttunnel.chat.build_requests" title="Permalink to this definition">ïƒ</a></dt>
<dd><p>Builds a list of ChatCompletionRequests from a list of texts.
The requests can then be passed to processor.process_api_requests().</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> â€“ The model to use for completion.</p></li>
<li><p><strong>function</strong> â€“ The function definition to use for the assistantâ€™s response.
Must be a dictionary that describes a valid JSON schema.
See <a class="reference external" href="https://platform.openai.com/docs/guides/gpt/function-calling">https://platform.openai.com/docs/guides/gpt/function-calling</a></p></li>
<li><p><strong>system_message</strong> â€“ The message to include at the beginning of each chat.</p></li>
<li><p><strong>texts</strong> â€“ A list of texts to binpack into chats.</p></li>
<li><p><strong>max_output_tokens</strong> â€“ The maximum number of tokens allowed in the completion.</p></li>
<li><p><strong>encoding_name</strong> â€“ The name of the encoding to use for tokenization.
Defaults to â€œcl100k_baseâ€.</p></li>
<li><p><strong>long_text_handling</strong> â€“ Passed to the binpacking function. Defaults to
â€œerrorâ€, which means that an error will be raised if a text is too
long to fit in a single chat.</p></li>
<li><p><strong>model_params</strong> â€“ Additional keyword arguments to pass to the OpenAI API. See
<a class="reference external" href="https://platform.openai.com/docs/api-reference/completions/create">https://platform.openai.com/docs/api-reference/completions/create</a></p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A list of ChatCompletionRequests.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="texttunnel.chat.is_valid_function_def">
<span class="sig-prename descclassname"><span class="pre">texttunnel.chat.</span></span><span class="sig-name descname"><span class="pre">is_valid_function_def</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">function</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#texttunnel.chat.is_valid_function_def" title="Permalink to this definition">ïƒ</a></dt>
<dd><p>Checks if a function definition is valid for use in a ChatCompletionRequest.
Note that the parameter properties are not validated to allow for custom properties.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>function</strong> â€“ The function definition to validate.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-texttunnel.models">
<span id="models-module"></span><h3>Models Module<a class="headerlink" href="#module-texttunnel.models" title="Permalink to this heading">ïƒ</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="texttunnel.models.Model">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">texttunnel.models.</span></span><span class="sig-name descname"><span class="pre">Model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">context_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_token_price_per_1k</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_token_price_per_1k</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokens_per_minute</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requests_per_minute</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#texttunnel.models.Model" title="Permalink to this definition">ïƒ</a></dt>
<dd><p>Information about an OpenAI ChatCompletion model.
Check prices here: <a class="reference external" href="https://openai.com/pricing">https://openai.com/pricing</a></p>
<p>Note that rate limits differ between OpenAI accounts.
Check them here: <a class="reference external" href="https://platform.openai.com/account/rate-limits">https://platform.openai.com/account/rate-limits</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> â€“ The name of the model, e.g. â€œgpt-3.5-turboâ€.</p></li>
<li><p><strong>context_size</strong> â€“ The maximum number of tokens that can be passed to the model.</p></li>
<li><p><strong>input_token_price_per_1k</strong> â€“ The price in USD per 1000 tokens for input.</p></li>
<li><p><strong>output_token_price_per_1k</strong> â€“ The price in USD per 1000 tokens for output.</p></li>
<li><p><strong>tokens_per_minute</strong> â€“ The maximum number of tokens that can be processed per minute.
Note that this may differ between OpenAI accounts. Override the default
modelsâ€™ values with your own values.</p></li>
<li><p><strong>requests_per_minute</strong> â€“ The maximum number of requests that can be made per minute.
Note that this may differ between OpenAI accounts. Override the default
modelsâ€™ values with your own values.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="texttunnel.models.GPT_4">
<span class="sig-prename descclassname"><span class="pre">models.</span></span><span class="sig-name descname"><span class="pre">GPT_4</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">Model(name='gpt-4',</span> <span class="pre">context_size=8192,</span> <span class="pre">input_token_price_per_1k=0.03,</span> <span class="pre">output_token_price_per_1k=0.06,</span> <span class="pre">tokens_per_minute=10000,</span> <span class="pre">requests_per_minute=200)</span></em><a class="headerlink" href="#texttunnel.models.GPT_4" title="Permalink to this definition">ïƒ</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="texttunnel.models.GPT_4_0613">
<span class="sig-prename descclassname"><span class="pre">models.</span></span><span class="sig-name descname"><span class="pre">GPT_4_0613</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">Model(name='gpt-4-0613',</span> <span class="pre">context_size=8192,</span> <span class="pre">input_token_price_per_1k=0.03,</span> <span class="pre">output_token_price_per_1k=0.06,</span> <span class="pre">tokens_per_minute=10000,</span> <span class="pre">requests_per_minute=200)</span></em><a class="headerlink" href="#texttunnel.models.GPT_4_0613" title="Permalink to this definition">ïƒ</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="texttunnel.models.GPT_4_32K">
<span class="sig-prename descclassname"><span class="pre">models.</span></span><span class="sig-name descname"><span class="pre">GPT_4_32K</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">Model(name='gpt-4-32k',</span> <span class="pre">context_size=32768,</span> <span class="pre">input_token_price_per_1k=0.06,</span> <span class="pre">output_token_price_per_1k=0.12,</span> <span class="pre">tokens_per_minute=20000,</span> <span class="pre">requests_per_minute=200)</span></em><a class="headerlink" href="#texttunnel.models.GPT_4_32K" title="Permalink to this definition">ïƒ</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="texttunnel.models.GPT_4_32K_0613">
<span class="sig-prename descclassname"><span class="pre">models.</span></span><span class="sig-name descname"><span class="pre">GPT_4_32K_0613</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">Model(name='gpt-4-32k-0613',</span> <span class="pre">context_size=32768,</span> <span class="pre">input_token_price_per_1k=0.06,</span> <span class="pre">output_token_price_per_1k=0.12,</span> <span class="pre">tokens_per_minute=20000,</span> <span class="pre">requests_per_minute=200)</span></em><a class="headerlink" href="#texttunnel.models.GPT_4_32K_0613" title="Permalink to this definition">ïƒ</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="texttunnel.models.GPT_4_0314">
<span class="sig-prename descclassname"><span class="pre">models.</span></span><span class="sig-name descname"><span class="pre">GPT_4_0314</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">Model(name='gpt-4-0314',</span> <span class="pre">context_size=8192,</span> <span class="pre">input_token_price_per_1k=0.03,</span> <span class="pre">output_token_price_per_1k=0.06,</span> <span class="pre">tokens_per_minute=10000,</span> <span class="pre">requests_per_minute=200)</span></em><a class="headerlink" href="#texttunnel.models.GPT_4_0314" title="Permalink to this definition">ïƒ</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="texttunnel.models.GPT_4_32K_0314">
<span class="sig-prename descclassname"><span class="pre">models.</span></span><span class="sig-name descname"><span class="pre">GPT_4_32K_0314</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">Model(name='gpt-4-32k-0314',</span> <span class="pre">context_size=32768,</span> <span class="pre">input_token_price_per_1k=0.06,</span> <span class="pre">output_token_price_per_1k=0.12,</span> <span class="pre">tokens_per_minute=10000,</span> <span class="pre">requests_per_minute=200)</span></em><a class="headerlink" href="#texttunnel.models.GPT_4_32K_0314" title="Permalink to this definition">ïƒ</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="texttunnel.models.GPT_3_5_TURBO">
<span class="sig-prename descclassname"><span class="pre">models.</span></span><span class="sig-name descname"><span class="pre">GPT_3_5_TURBO</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">Model(name='gpt-3.5-turbo',</span> <span class="pre">context_size=4096,</span> <span class="pre">input_token_price_per_1k=0.0015,</span> <span class="pre">output_token_price_per_1k=0.002,</span> <span class="pre">tokens_per_minute=90000,</span> <span class="pre">requests_per_minute=3500)</span></em><a class="headerlink" href="#texttunnel.models.GPT_3_5_TURBO" title="Permalink to this definition">ïƒ</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="texttunnel.models.GPT_3_5_TURBO_16K">
<span class="sig-prename descclassname"><span class="pre">models.</span></span><span class="sig-name descname"><span class="pre">GPT_3_5_TURBO_16K</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">Model(name='gpt-3.5-turbo-16k',</span> <span class="pre">context_size=16384,</span> <span class="pre">input_token_price_per_1k=0.003,</span> <span class="pre">output_token_price_per_1k=0.004,</span> <span class="pre">tokens_per_minute=180000,</span> <span class="pre">requests_per_minute=3500)</span></em><a class="headerlink" href="#texttunnel.models.GPT_3_5_TURBO_16K" title="Permalink to this definition">ïƒ</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="texttunnel.models.GPT_3_5_TURBO_0613">
<span class="sig-prename descclassname"><span class="pre">models.</span></span><span class="sig-name descname"><span class="pre">GPT_3_5_TURBO_0613</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">Model(name='gpt-3.5-turbo-0613',</span> <span class="pre">context_size=4096,</span> <span class="pre">input_token_price_per_1k=0.0015,</span> <span class="pre">output_token_price_per_1k=0.002,</span> <span class="pre">tokens_per_minute=90000,</span> <span class="pre">requests_per_minute=3500)</span></em><a class="headerlink" href="#texttunnel.models.GPT_3_5_TURBO_0613" title="Permalink to this definition">ïƒ</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="texttunnel.models.GPT_3_5_TURBO_16K_0613">
<span class="sig-prename descclassname"><span class="pre">models.</span></span><span class="sig-name descname"><span class="pre">GPT_3_5_TURBO_16K_0613</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">Model(name='gpt-3.5-turbo-16k-0613',</span> <span class="pre">context_size=16384,</span> <span class="pre">input_token_price_per_1k=0.003,</span> <span class="pre">output_token_price_per_1k=0.004,</span> <span class="pre">tokens_per_minute=180000,</span> <span class="pre">requests_per_minute=3500)</span></em><a class="headerlink" href="#texttunnel.models.GPT_3_5_TURBO_16K_0613" title="Permalink to this definition">ïƒ</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="texttunnel.models.GPT_3_5_TURBO_0301">
<span class="sig-prename descclassname"><span class="pre">models.</span></span><span class="sig-name descname"><span class="pre">GPT_3_5_TURBO_0301</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">Model(name='gpt-3.5-turbo-0301',</span> <span class="pre">context_size=4096,</span> <span class="pre">input_token_price_per_1k=0.0015,</span> <span class="pre">output_token_price_per_1k=0.002,</span> <span class="pre">tokens_per_minute=9000,</span> <span class="pre">requests_per_minute=3500)</span></em><a class="headerlink" href="#texttunnel.models.GPT_3_5_TURBO_0301" title="Permalink to this definition">ïƒ</a></dt>
<dd></dd></dl>

</section>
<section id="module-texttunnel.processor">
<span id="processor-module"></span><h3>Processor Module<a class="headerlink" href="#module-texttunnel.processor" title="Permalink to this heading">ïƒ</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="texttunnel.processor.APIRequest">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">texttunnel.processor.</span></span><span class="sig-name descname"><span class="pre">APIRequest</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">task_id:</span> <span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">request:</span> <span class="pre">~texttunnel.chat.ChatCompletionRequest</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">token_consumption:</span> <span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attempts_left:</span> <span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">result:</span> <span class="pre">list</span> <span class="pre">=</span> <span class="pre">&lt;factory&gt;</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#texttunnel.processor.APIRequest" title="Permalink to this definition">ïƒ</a></dt>
<dd><p>Stores an API requestâ€™s inputs, outputs, and other metadata.
Contains a method to make an API call.</p>
<dl class="py method">
<dt class="sig sig-object py" id="texttunnel.processor.APIRequest.call_api">
<em class="property"><span class="pre">async</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">call_api</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">request_url</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">request_header</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry_queue</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Queue</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_filepath</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">status_tracker</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#texttunnel.processor.StatusTracker" title="texttunnel.processor.StatusTracker"><span class="pre">StatusTracker</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Cache</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#texttunnel.processor.APIRequest.call_api" title="Permalink to this definition">ïƒ</a></dt>
<dd><p>Calls the OpenAI API and appends the request and result to a JSONL file.
If a cache provided, the result will be stored in the cache.
The cache key is the hash of the request.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>request_url</strong> â€“ The URL to send the request to.</p></li>
<li><p><strong>request_header</strong> â€“ The header to send with the request.</p></li>
<li><p><strong>retry_queue</strong> â€“ A queue of requests that need to be retried.
Will be populated if the request fails.</p></li>
<li><p><strong>output_filepath</strong> â€“ The path to the file where the results will be saved.</p></li>
<li><p><strong>status_tracker</strong> â€“ A StatusTracker object that tracks the greater
request loopâ€™s progress.</p></li>
<li><p><strong>cache</strong> â€“ A diskcache.Cache object to store API responses in. Optional.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="texttunnel.processor.StatusTracker">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">texttunnel.processor.</span></span><span class="sig-name descname"><span class="pre">StatusTracker</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_tasks_started</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_tasks_in_progress</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_tasks_succeeded</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_tasks_failed</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_rate_limit_errors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_api_errors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_other_errors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_of_last_rate_limit_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#texttunnel.processor.StatusTracker" title="Permalink to this definition">ïƒ</a></dt>
<dd><p>Stores metadata about the scriptâ€™s progress. Only one instance is created.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="texttunnel.processor.append_to_jsonl">
<span class="sig-prename descclassname"><span class="pre">texttunnel.processor.</span></span><span class="sig-name descname"><span class="pre">append_to_jsonl</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filename</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Path</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#texttunnel.processor.append_to_jsonl" title="Permalink to this definition">ïƒ</a></dt>
<dd><p>Append a json payload to the end of a jsonl file.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> â€“ The data to append.</p></li>
<li><p><strong>filename</strong> â€“ The file to append to.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="texttunnel.processor.aprocess_api_requests">
<em class="property"><span class="k"><span class="pre">async</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">texttunnel.processor.</span></span><span class="sig-name descname"><span class="pre">aprocess_api_requests</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">requests</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#texttunnel.chat.ChatCompletionRequest" title="texttunnel.chat.ChatCompletionRequest"><span class="pre">ChatCompletionRequest</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_filepath</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_attempts</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rate_limit_headroom_factor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">api_key</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Cache</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#texttunnel.processor.aprocess_api_requests" title="Permalink to this definition">ïƒ</a></dt>
<dd><p>Processes API requests in parallel, throttling to stay under rate limits.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="texttunnel.processor.is_valid_response">
<span class="sig-prename descclassname"><span class="pre">texttunnel.processor.</span></span><span class="sig-name descname"><span class="pre">is_valid_response</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">response</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">print_errors</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#texttunnel.processor.is_valid_response" title="Permalink to this definition">ïƒ</a></dt>
<dd><p>Check if a response conforms to the response JSON schema.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="texttunnel.processor.parse_arguments">
<span class="sig-prename descclassname"><span class="pre">texttunnel.processor.</span></span><span class="sig-name descname"><span class="pre">parse_arguments</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">response</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#texttunnel.processor.parse_arguments" title="Permalink to this definition">ïƒ</a></dt>
<dd><p>Extract the function call arguments from a response.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>response</strong> â€“ The response to parse. It should be a list of length 2, where the
first element is the request and the second element is the response.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The function call arguments.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="texttunnel.processor.parse_token_usage">
<span class="sig-prename descclassname"><span class="pre">texttunnel.processor.</span></span><span class="sig-name descname"><span class="pre">parse_token_usage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">response</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#texttunnel.processor.parse_token_usage" title="Permalink to this definition">ïƒ</a></dt>
<dd><p>Extract the token usage from a response.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>response</strong> â€“ The response to parse. It should be a list of length 2, where the
first element is the request and the second element is the response.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The token usage.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="texttunnel.processor.prepare_output_filepath">
<span class="sig-prename descclassname"><span class="pre">texttunnel.processor.</span></span><span class="sig-name descname"><span class="pre">prepare_output_filepath</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_filepath</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Path</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_file</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Path</span></span></span><a class="headerlink" href="#texttunnel.processor.prepare_output_filepath" title="Permalink to this definition">ïƒ</a></dt>
<dd><p>Validates the output_filepath and returns a Path object. Uses a temporary file
if output_filepath is None.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_filepath</strong> â€“ The path to save the results to. If None, a temporary file
will be used.</p></li>
<li><p><strong>keep_file</strong> â€“ Whether to keep the file after the function returns. If True,
output_filepath must not be None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A Path object representing the output_filepath.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="texttunnel.processor.process_api_requests">
<span class="sig-prename descclassname"><span class="pre">texttunnel.processor.</span></span><span class="sig-name descname"><span class="pre">process_api_requests</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">requests</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#texttunnel.chat.ChatCompletionRequest" title="texttunnel.chat.ChatCompletionRequest"><span class="pre">ChatCompletionRequest</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_filepath</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Path</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_file</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_attempts</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rate_limit_headroom_factor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.75</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">api_key</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Cache</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#texttunnel.processor.process_api_requests" title="Permalink to this definition">ïƒ</a></dt>
<dd><p>Using the OpenAI API to process lots of text quickly takes some care.
If you trickle in a million API requests one by one, theyâ€™ll take days to complete.
If you flood a million API requests in parallel, theyâ€™ll exceed the rate limits
and fail with errors. To maximize throughput, parallel requests need to be
throttled to stay under rate limits.</p>
<p>The following functions parallelizes requests to the OpenAI API while
throttling to stay under rate limits.</p>
<p>Features:
- Streams requests from file, to avoid running out of memory for giant jobs
- Makes requests concurrently, to maximize throughput
- Throttles request and token usage, to stay under rate limits
- Retries failed requests up to {max_attempts} times, to avoid missing data
- Logs errors, to diagnose problems with requests</p>
<p>Processes API requests in parallel, throttling to stay under rate limits.
This function is a wrapper for aprocess_api_requests() that runs it in an asyncio
event loop. Also sorts the output by request ID, so that the results are in
the same order as the requests.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>requests</strong> â€“ List[ChatCompletionRequest]
The requests to process, see ChatCompletionRequest class for details</p></li>
<li><p><strong>output_filepath</strong> â€“ str, optional
Path to the file where the results will be saved
file will be a jsonl file, where each line is an array with the original
request plus the API response e.g.,
[{â€œmodelâ€: â€œgpt-4â€, â€œmessagesâ€: â€œâ€¦â€}, {â€¦}]
if omitted, the results will be saved to a temporary file.</p></li>
<li><p><strong>keep_file</strong> â€“ bool, optional
Whether to keep the results file after the script finishes, in addition
to the results being returned by the function.
Defaults to False, so the file will be deleted after the script finishes.
Setting this to True is not compatible with output_filepath=None.</p></li>
<li><p><strong>max_attempts</strong> â€“ int, optional
Number of times to retry a failed request before giving up
if omitted, will default to 5</p></li>
<li><p><strong>rate_limit_headroom_factor</strong> â€“ float, optional
Factor to multiply the rate limit by to guarantee that the script
stays under the limit if omitted, will default to 0.75
(75% of the rate limit)</p></li>
<li><p><strong>api_key</strong> â€“ str, optional
API key to use
if omitted, the function will attempt to read it from an environment
variable {os.getenv(â€œOPENAI_API_KEYâ€)}
If all requests are cached, the API key is not required.</p></li>
<li><p><strong>cache</strong> â€“ diskcache.Cache, optional
If provided, API responses will be served from the cache if available.
New responses will be saved to the cache.
Create a cache object with <cite>diskcache.Cache(â€œpath/to/cacheâ€)</cite></p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>list where each element consists of two dictionaries:</dt><dd><ul class="simple">
<li><p>the original request</p></li>
<li><p>the API response</p></li>
</ul>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>List[Dict[str, Any]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="texttunnel.processor.task_id_generator_function">
<span class="sig-prename descclassname"><span class="pre">texttunnel.processor.</span></span><span class="sig-name descname"><span class="pre">task_id_generator_function</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Generator</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#texttunnel.processor.task_id_generator_function" title="Permalink to this definition">ïƒ</a></dt>
<dd><p>Generate integers 0, 1, 2, and so on.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A generator that yields integers.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="texttunnel.processor.usage_to_cost">
<span class="sig-prename descclassname"><span class="pre">texttunnel.processor.</span></span><span class="sig-name descname"><span class="pre">usage_to_cost</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">usage</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#texttunnel.models.Model" title="texttunnel.models.Model"><span class="pre">Model</span></a></span></em><span class="sig-paren">)</span><a class="headerlink" href="#texttunnel.processor.usage_to_cost" title="Permalink to this definition">ïƒ</a></dt>
<dd><p>Convert token usage to cost in USD.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>usage</strong> â€“ The token usage. Retrieve it with parse_token_usage().</p></li>
<li><p><strong>model</strong> â€“ The model used to generate the response.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The cost in USD.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-texttunnel.utils">
<span id="utils-module"></span><h3>Utils Module<a class="headerlink" href="#module-texttunnel.utils" title="Permalink to this heading">ïƒ</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="texttunnel.utils.binpack_texts_in_order">
<span class="sig-prename descclassname"><span class="pre">texttunnel.utils.</span></span><span class="sig-name descname"><span class="pre">binpack_texts_in_order</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">texts</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">formatter_function</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_tokens_per_bin</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_texts_per_bin</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoding_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'cl100k_base'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">long_text_handling</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'error'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#texttunnel.utils.binpack_texts_in_order" title="Permalink to this definition">ïƒ</a></dt>
<dd><p>Binpacks a list of texts into a list of lists of texts, such that each list of texts
has a total number of tokens less than or equal to max_tokens_per_bin and each list of texts
has a number of texts less than or equal to max_texts_per_bin.</p>
<p>The binpacking uses a naive greedy algorithm that maintains the order of the texts.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>texts</strong> â€“ List of texts to binpack. Empty texts are accepted, counted as 0 tokens
each and count against max_texts_per_bin.</p></li>
<li><p><strong>formatter_function</strong> â€“ A function that takes a list of texts and returns a single
text. Defaults to None, which means that the texts are joined with spaces.
This function is used to include the overhead of the formatter function in
the binpacking. It is not used to format the output. Make sure to use
the same formatter function when formatting the output for the model.</p></li>
<li><p><strong>max_tokens_per_bin</strong> â€“ The maximum number of tokens per bin of formatted texts.
Leave some room for relative to the modelâ€™s context size to account for the tokens in the
system message, function call, and function return.</p></li>
<li><p><strong>max_texts_per_bin</strong> â€“ The maximum number of texts per list of texts. Defaults to None, which
means that there is no limit on the number of texts per list of texts.</p></li>
<li><p><strong>encoding_name</strong> â€“ The name of the encoding to use. Defaults to â€œcl100k_baseâ€.</p></li>
<li><p><strong>long_text_handling</strong> â€“ How to handle texts that are longer than max_tokens_per_bin. Defaults
to â€œerrorâ€, which means that an error is raised. Can also be set to
â€œtruncateâ€, which means that the text is truncated to max_tokens_per_bin.
It is possible that more tokens are truncated than absolutely necessary
due to overhead of the formatter function caused by escaping characters.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A list of lists of texts. The order of the texts is preserved.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="texttunnel.utils.format_texts_as_json">
<span class="sig-prename descclassname"><span class="pre">texttunnel.utils.</span></span><span class="sig-name descname"><span class="pre">format_texts_as_json</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">texts</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#texttunnel.utils.format_texts_as_json" title="Permalink to this definition">ïƒ</a></dt>
<dd><p>Formats a list of texts into a single string to be used as a user message.
Each text is assigned an ID, starting from 0. The returned JSON format
helps the model distinguish between different texts, at the cost of
increasing the number of tokens used.</p>
<p>The token overhead for a single text that doesnâ€™t require escaping characters
is 12 tokens. Escaping characters like quotes increases the overhead.</p>
<p>The format is a JSON list of dictionaries, where each dictionary has an
â€œidâ€ key and a â€œtextâ€ key. The â€œidâ€ key is an integer, and the â€œtextâ€ key
is a string. This array of maps structure is easiest to parse by GPT models
and handles edge cases like newlines in the text.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>texts</strong> â€“ A list of texts to format.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A formatted string that can be used as a user message.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="texttunnel.utils.format_texts_with_spaces">
<span class="sig-prename descclassname"><span class="pre">texttunnel.utils.</span></span><span class="sig-name descname"><span class="pre">format_texts_with_spaces</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">texts</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#texttunnel.utils.format_texts_with_spaces" title="Permalink to this definition">ïƒ</a></dt>
<dd><p>Simple formatter that joins texts with spaces.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="texttunnel.utils.hash_dict">
<span class="sig-prename descclassname"><span class="pre">texttunnel.utils.</span></span><span class="sig-name descname"><span class="pre">hash_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#texttunnel.utils.hash_dict" title="Permalink to this definition">ïƒ</a></dt>
<dd><p>Hashes a dictionary using sha256.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="texttunnel.utils.num_tokens_from_text">
<span class="sig-prename descclassname"><span class="pre">texttunnel.utils.</span></span><span class="sig-name descname"><span class="pre">num_tokens_from_text</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoding_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'cl100k_base'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#texttunnel.utils.num_tokens_from_text" title="Permalink to this definition">ïƒ</a></dt>
<dd><p>Returns the number of tokens in a string.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>text</strong> â€“ The text to count tokens in.</p></li>
<li><p><strong>encoding_name</strong> â€“ The name of the token encoding to use. Defaults to â€œcl100k_baseâ€.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The number of tokens in the string.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="texttunnel.utils.truncate_text_by_tokens">
<span class="sig-prename descclassname"><span class="pre">texttunnel.utils.</span></span><span class="sig-name descname"><span class="pre">truncate_text_by_tokens</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoding</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Encoding</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#texttunnel.utils.truncate_text_by_tokens" title="Permalink to this definition">ïƒ</a></dt>
<dd><p>Truncates a text to a maximum number of tokens.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>text</strong> â€“ The text to truncate.</p></li>
<li><p><strong>max_tokens</strong> â€“ The maximum number of tokens to truncate the text to.</p></li>
<li><p><strong>encoding</strong> â€“ The encoding to use.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The truncated text.</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
</section>
<section id="indices-and-tables">
<h1>Indices and tables<a class="headerlink" href="#indices-and-tables" title="Permalink to this heading">ïƒ</a></h1>
<ul class="simple">
<li><p><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></p></li>
<li><p><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></p></li>
<li><p><a class="reference internal" href="search.html"><span class="std std-ref">Search Page</span></a></p></li>
</ul>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Q Agentur fÃ¼r Forschung GmbH.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>